{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/In_class_exercise/In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EuX00KHNeSpw"
   },
   "source": [
    "# **The fourth in-class-exercise (20 points in total, 2/9/2021)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-vTOb03hG1f"
   },
   "source": [
    "# 1. Text Data Preprocessing\n",
    "\n",
    "Here is a [legal case](https://github.com/unt-iialab/info5731_spring2021/blob/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
    "\n",
    "\n",
    "\n",
    "## 1.1 Basic feature extraction using text data (4 points)\n",
    "\n",
    "*   Number of sentences\n",
    "*   Number of words\n",
    "*   Number of characters\n",
    "*   Average word length\n",
    "*   Number of stopwords\n",
    "*   Number of special characters\n",
    "*   Number of numerics\n",
    "*   Number of uppercase words\n",
    "\n",
    "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
    "\n",
    "*   Lower casing\n",
    "*   Punctuation removal\n",
    "*   Stopwords removal\n",
    "*   Frequent words removal\n",
    "*   Rare words removal\n",
    "*   Spelling correction\n",
    "*   Tokenization\n",
    "*   Stemming\n",
    "*   Lemmatization\n",
    "\n",
    "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
    "\n",
    "\n",
    "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
    "\n",
    "*   Calculate the term frequency of all the terms.\n",
    "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vR0L3_CreM_A"
   },
   "outputs": [],
   "source": [
    "number_of_words = 0\n",
    "num_of_characters = 0\n",
    "total = 0\n",
    "count = 0\n",
    "count_special = 0\n",
    "total_numerics = 0\n",
    "uppercase_count = 0\n",
    "upper = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "         'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "\n",
    "with open('legal_case.txt') as f:\n",
    "    text = f.read()\n",
    "    # print(text)\n",
    "\n",
    "    # count number of words\n",
    "    for line in text:\n",
    "        lines = line.strip(\"\\n\")\n",
    "        words = lines.split()\n",
    "        number_of_words += len(words)\n",
    "        count += 1\n",
    "\n",
    "    # count number of characters\n",
    "        num_of_characters += len(lines)\n",
    "\n",
    "    # Average word length\n",
    "        Average = number_of_words / count\n",
    "\n",
    "        # Number of special characters\n",
    "\n",
    "        count_special += sum(not x.isalnum() for x in line)\n",
    "\n",
    "        # Number of numerics characters\n",
    "        numbers = []\n",
    "        count = 0\n",
    "        for i in text.split():\n",
    "            if i.isdigit():\n",
    "                numbers.append(int(i))\n",
    "            count = +1\n",
    "\n",
    "        # Number of Uppercase\n",
    "\n",
    "    for character in text:\n",
    "        if character in upper:\n",
    "            uppercase_count += 1\n",
    "\n",
    "\n",
    "    print(\"******OUTPUT*******\\n\")\n",
    "    print(\"The numbers of words in the given file is :\", number_of_words)\n",
    "    print(\"The numbers of characters in the given file is :\", num_of_characters)\n",
    "    print(\"The average word length in the given file is :\", Average)\n",
    "    print(\"The number of special character in a file:\", count_special)\n",
    "    print(\"The numerics value found in the given file is:\", numbers,\n",
    "          \"\\n And the total count of numerics in the text is:\", len(numbers))\n",
    "    print('The uppercase count is', uppercase_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nlp as nlp\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\"\"\"df = pd.read_csv(\"legal_case.csv\")\n",
    "df.head()\"\"\"\n",
    "\n",
    "with open('legal_case.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "# lower casing\n",
    "def text_lowercase(change):\n",
    "    return change.lower()\n",
    "\n",
    "\n",
    "# punctuation removal\n",
    "def remove_punctuation(change1):\n",
    "    remove = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(remove)\n",
    "\n",
    "\n",
    "# stopwords removal\n",
    "def stopwords_removal(change2):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "# stemming\n",
    "def stemming(change3):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    for word in word_tokens:\n",
    "        stems.append(stemmer.stem(word))\n",
    "    return stems\n",
    "\n",
    "\n",
    "# Spelling correction\n",
    "def spelling(change4):\n",
    "    textBlb = TextBlob(text)  # Making our first textblob\n",
    "    textCorrected = textBlb.correct()  # Correcting the text\n",
    "    return textCorrected\n",
    "\n",
    "\n",
    "# tokenization\n",
    "def tokenize(x):\n",
    "    text_tokenize = TextBlob(x).words\n",
    "    doc = nlp(x)\n",
    "    for token in doc:\n",
    "        return token\n",
    "\n",
    "\n",
    "# call the functions\n",
    "\"\"\"print(text_lowercase(text))\n",
    "print(remove_punctuation(text))\n",
    "print(stopwords_removal(text))\n",
    "print(stemming(text))\n",
    "print(spelling(text))\"\"\"\n",
    "print(tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BBiC4E_kefvV"
   },
   "source": [
    "# 2. Python Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1QJ-UwCenvN"
   },
   "source": [
    "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
    "\n",
    "ip = \"260.08.094.109\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wSv6fVhOfFmv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IP address value after removing the leading zero's is:\n",
      "260.8.94.109\n"
     ]
    }
   ],
   "source": [
    "def remove_leading_zeros(ip):\n",
    "    new_ip = \".\".join([str(int(i)) for i in ip.split(\".\")])\n",
    "    return new_ip\n",
    "ip_add=\"260.08.094.109\"\n",
    "print(\"The IP address value after removing the leading zero's is:\")\n",
    "print(remove_leading_zeros(ip_add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXRjaHzrfKAy"
   },
   "source": [
    "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
    "\n",
    "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xdJpDx9gjbX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The years value in the given sentence are:\n",
      " ['2010', '2010', '2019']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def find_years(s):\n",
    "    find = re.findall(r\"[\\d]{4}\", sentence)  # matching the dates with 4 digits\n",
    "\n",
    "    new = []\n",
    "    for years in find:\n",
    "        if int(years) > 1900:\n",
    "            new.append(years)\n",
    "    print(\"The years value in the given sentence are:\\n\", new)\n",
    "\n",
    "\n",
    "sentence = \"\"\"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have ' \\\n",
    "                        'happened. As the decade comes to a close, Insider took a look back at some of the biggest ' \\\n",
    "                        'headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from ' \\\n",
    "                        'the heartwarming rescue of a Thai boys soccer team from a flooded cave to the divisive election ' \\\n",
    "                        'of President Donald Trump.  \"\"\"\n",
    "find_years(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPBbHN93zlE4UST77Gx/JWu",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "In-class-exercise-04.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
